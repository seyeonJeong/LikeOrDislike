# LikeOrDislike


### 사용 데이터셋

---

네이버 영화 리뷰 데이터셋 - https://github.com/e9t/nsmc



### Introduction

---

간단한 NLP를 배우던 중 https://word2vec.kr/search/ 라는 사이트에서 Word2Vec의 원리를 통해 단어를 연산하는 예시를 보고 영감을 얻어 진행한 프로젝트이다.

영화에서 좋아하는 장르를 입력하고, 싫어하는 장르를 입력하면, 좋아하는 장르 - 싫어하는 장르 의 연산 값이 출력되는 간단한 프로그램이다.

우선, Word2Vec의 원리를 알아보자

#### Word2Vec

내가 공부한 자연어처리과정에서는 단어를 토큰화시킨 후 BoW 기법으로 단어의 빈도수를 기반으로 인덱싱을 해준 다음에 그 인덱스를 원-핫 인코딩하는 과정으로 전처리를 해주었다.

이를 희소 표현 (Sparse Representation) 이라고 하며, 간단하게 말하면 벡터 또는 행렬이 대부분 0으로 표현되는 방법이다. 원-핫 벡터의 경우에도 하나를 제외한 대부분이 0이므로 희소 표현에 해당된다. 하지만 이러한 표현 방식은 각 단어간 유사성을 표현할 수 없다. 이를 위해 분산 표현 (Distributed Representation) 이라는 방식이 탄생하게 되었다. 단어의 의미를 벡터화 하는 작업으로 워드 임베딩 작업에 속하며 이러한 벡터를 임베딩 벡터라고 한다.

분산 표현은 비슷한 위치의 단어들이 비슷한 값을 가질 거라는 전제로 진행된다. 이를 통해 의미적으로 가깝다고 판단하는 것.

Word2Vec에는 CBOW(Continous Bag of Words)와 Skip-Gram의 방식이 있다. CBOW는 중심 단어를 주변 단어로 예측하는 방식이고, Skip-Gram은 중심 단어로 주변 단어를 예측하는 방법이다.

![cbow](https://github.com/seyeonJeong/LikeOrDislike/blob/main/CBOW.webp)

위 사진은 CBOW의 구조이다. 입력으로 들어온 원-핫 벡터와 가중치 W의 행렬의 곱을 진행하는데, 원핫 벡터이므로 1이라는 값을 가진 인덱스에 해당하는 가중치의 행이 곧 행렬의 곱의 결과가 된다. 원리만 본다면 가중치의 값이 결과에 많은 영향을 줄 수 밖에 없다.

이렇게 계산된 값은 은닉층에서 벡터들의 평균으로 구해지게 된다. 또한 출력층에서 소프트멕스 함수를 취하는데 이렇게 나온 확률값은 중심 단어의 원-핫 벡터의 값에 가까워지게 가중치를 갱신한다.

Skip-gram의 경우 비슷한 원리를 가지지만 중심 단어에서 주변 단어를 예측하기 때문에, 은닉층에서 벡터의 합의 평균을 구하지 않아도 된다. 이 떄문인지 전반적으로 Skip-gram이 CBOW보다 성능이 좋다고 한다.

Skip-gram 에서는 모든 단어들에 대해서 중심 - 주변 단어의 관계를 학습하므로 학습의 양이 많아진다. 예를 들어 10,000개의 단어에서 주변 단어 300개를 학습한다면 10,000 * 300 = 3,000,000번의 학습이 필요하다. 이를 통해 계산량이 굉장히 많아진다는 단점이 생기는데, 이를 Negative Sampling으로 해결할 수 있다. 

주변 단어인 (positive) 샘플과 주변 단어가 아닌 (negative) 샘플을 이용하게 되는데 이 샘플에 각각 (1,0)의 추가 레이블 작업이 필요하다. 이를 통하여 중심단어와 주변단어의 내적값과 레이블의 오차로부터 역전파하며 임베딩 테이블을 업데이트하는 과정을 거친다. 여기서 전체 단어가 아닌 일부 단어로 근사시키는 과정이므로, 샘플링 이라는 단어로 표현되었다. 또한, skip-gram이 다중분류를 사용한 것과 다르게 SGNS는 긍정과 부정으로만 답을 내린다. (이 단어가 중심 단어가 맞는가? yes or no)(추후 프로젝트에 추가 예정)


### Code

---


Reference - https://wikidocs.net/217114

위 사이트를 참고하여, 프로젝트를 진행하였다.

![preprocessing](https://github.com/seyeonJeong/LikeOrDislike/blob/main/preprocessing_code.PNG)


위 사진에서는 네이버 리뷰 데이터셋을 전처리하는 과정의 코드이다.

우선 Null값을 제거해주는데 처음에 저 과정을 하지 않았더니, 토큰화 과정에서 에러가 발생한다. 자연어처리 프로젝트에서 데이터셋을 다룰 때 필수적인 부분이라고 생각한다.

또한 정규 표현식을 통하여 한글 외 문자를 제거해 주었다. 이 과정은 필요 목적에 따라 달라질 수 있다고 생각한다.

불용어를 정의해주고 불용어는 제거해주는 과정을 거쳤는데 내가 사용한 konlpy의 토큰화 과정에서 조사와 단어를 분리해주는 기능이 있는 것 같다. 그래서 저러한 의미와 큰 상관이 없는 불용어를 정의해놓고 토큰화 과정에서 걸러주는 과정을 거친다. tqdm 이라는 함수를 사용했는데 단순히 진행률을 프로세스 바로 표현해주는 기능을 하는 함수이다.


![result](https://github.com/seyeonJeong/LikeOrDislike/blob/main/result.PNG)


Word2Vec으로 전처리된 데이터를 학습시킨다. 여기서 window는 Introduction에서 설명하였듯, 주변 단어의 개수이다. 윈도우 개수를 바탕으로 윈도우 슬라이싱을 진행하며 학습이 진행된다. 

임베딩 메트릭스는 16477개의 단어 100개의 차원으로 이루어진 것을 볼 수 있다.


마지막으로 결과값을 출력하는 코드에서는 좋아하는 장르, 싫어하는 장르를 입력하면 해당 단어를 토큰화 시켜 가장 비슷한 결과값의 상위 5개를 출력시켜 주었다. 주제를 장르로 고정시켰지만 고정시키지 않는다면 positive에 남자 + 여배우 negative에 배우 라는 값을 입력하면 여자와 관련된 결과값이 나오는 예시도 존재한다.


### Result

---

사실 CNN과 이미지 처리쪽은 졸업작품을 거치며 어느정도 지식이 있고 자주 접했던 개념이 많았는데 자연어처리 분야는 이번에 처음 공부하게 되어 생소한 단어가 많고, 초반에 개념이 잘 이해가 되지 않았다. 단어와 단어의 연산으로 우리가 익히 상상할 수 있는 결과가 나오는것이 신기했고, 앞으로 Negative Sampling 이나 BERT와 같은 다양한 모델을 적용한 프로젝트 또한 진행할 예정이다.

또한, 추후 다양한 데이터셋과 모델을 추가한다면 영화 추천 모델까지도 발전시킬 가능성이 있을 것으로 예상된다.


